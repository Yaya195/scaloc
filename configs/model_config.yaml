encoder:
  num_aps: 521  # Must be > max AP ID (WAP001-WAP520 -> 1-520)
  latent_dim: 64  # AP encoder output dimension
  ap_emb_dim: 32  # AP embedding dimension
  pooling: attention  # 'mean', 'sum', or 'attention'
  hidden_dim: 128
  dropout: 0.2
  activation: relu
  
gnn:
  type: graphsage
  arch: sage  # 'sage' or 'gat'
  num_layers: 3
  hidden_dim: 128
  output_dim: 2
  
fusion:
  enabled: false

baselines:
  knn_k: 5
  mlp_hidden_dim: 128
  mlp_num_layers: 3
  mlp_dropout: 0.2
